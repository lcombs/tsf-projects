---
title: "Project 2: Forecasting NYC Noise Complaints"
author: "Elizabeth Combs (eac721)"
date: "05/04/2021"
output:
  pdf_document:
    toc: no
urlcolor: blue
---

```{r setup, include=FALSE, echo=FALSE}
library(webshot)
webshot::install_phantomjs()
knitr::opts_chunk$set(echo = FALSE, dev='pdf') 
library("forecast")
library('reshape2')
library('plotly')
library("ggplot2")
library('lubridate')
library("RSocrata")
library('dplyr')
library('magrittr')
library("tseries")
library('DT')
```

# 0. Introduction

The data for this project was retrieved from NYC Open Data which hosts all of the [311 Service Requests from 2010 to Present](https://data.cityofnewyork.us/Social-Services/311-Service-Requests-from-2010-to-Present/erm2-nwe9). The data is filtered to create about five years of data from Jan. 1, 2016 through Apr. 30, 2021 for "Noise - Residential" complaint types to reduce the size of the data pulled from the API and focus on a shorter, more recent period of time. The data was filtered using this [documentation](https://dev.socrata.com/docs/filtering.html). There are a total of 1,410,942 complaints during this time period with an average of 724 complaints per day. A simple tally (sum by date) of the complaints was used for this project so that the data could be converted to a daily time series for modeling purposes to make evenly spaced time intervals. *Note: All code for this project is available on [GitHub](https://github.com/lcombs/tsf-projects/blob/main/project02/project02_eac721.Rmd).* <br/>


```{r, echo=F, cache=TRUE}

# Read in data:
df <- read.socrata(url="https://data.cityofnewyork.us/resource/erm2-nwe9.json?$where=created_date between '2016-01-01T00:00:00.000' and '2021-04-30T00:00:00.000' AND complaint_type='Noise - Residential'") # AND city='NEW YORK' AND borough='MANHATTAN'

#head(df)

df$Date <- as.Date(df$created_date)

# group number of complaints by date
noise_complaints <- df %>% group_by(Date) %>% tally
#data.frame(df %>% group_by(Date) %>% tally)$n %>% mean(., na.rm=T)

# group by week 
# noise_complaints <- df %>% group_by(week = format(Date, '%Y-%U')) %>% tally %>% as.data.frame()
# noise_complaints$week <- as.Date(paste0(noise_complaints$week, '-1'),'%Y-%U-%u')

# four plot variables
noise_complaints$log_n <- log(noise_complaints$n)
noise_complaints$n_diff <- c(NA, diff(noise_complaints$n))
noise_complaints$log_n_diff <- c(NA, diff(noise_complaints$log_n))
noise_complaints$n_diff2 <- c(NA, diff(noise_complaints$n_diff))


```

# 1. Data Review: Take Logs, Take Differences

```{r, echo=FALSE, fig.width=8, fig.height=5}
# four plots
par(mfrow=c(2,2))
# series, diff series
plot(noise_complaints$Date, noise_complaints$n, type = 'l', main = 'Daily Noise Complaints', xlab = 'Date', ylab='Number of Noise Complaints', col='red')
plot(noise_complaints$Date, noise_complaints$n_diff, type = 'l',  main = 'Daily Diff. Noise Complaints', xlab = 'Date', ylab='Diff. Noise Complaints', col='red')
# log series, diff log series
plot(noise_complaints$Date, noise_complaints$log_n, type = 'l',  main = 'Log Daily Noise Complaints', xlab = 'Date', ylab='Log Noise Complaints', col='red')
plot(noise_complaints$Date, noise_complaints$log_n_diff, type = 'l',  main = 'Log Diff. Daily Noise Complaints', xlab = 'Date', ylab='Log Diff. Noise Complaints', col='red')


```
Based on the graphs above, we can see the level-dependent volatility in the upper two plots. Thus, we will log the data so that we can eliminate the level-dependent volatility (which we can see in the lower graphs). <br/>

Additionally, there appears to be a weekly seasonality component of the residential noise complaints with larger values on weekends. We will remove the weekly seasonality as described in Project 1 by subtracting the seasonal mean from each value to remove this structure. <br/>


```{r}
# remove seasonality (weekly)
noise_complaints$day <- weekdays(noise_complaints$Date)
seasonal_means <- aggregate(noise_complaints$log_n, list(noise_complaints$day), mean, na.rm=T) %>% as.data.frame()
seasonal_means$x<-round(seasonal_means$x, 2)
colnames(seasonal_means)<-c('day', 'log_weekly_noise_complaints')
datatable(seasonal_means, options = list(pageLength = 7))
# subtract the seasonal mean (weekly)
noise_complaints<-merge(noise_complaints, seasonal_means, by.x = 'day',by.y = 'day')
noise_complaints$log_n_adj <- noise_complaints$log_n - noise_complaints$log_weekly_noise_complaints
noise_complaints <- noise_complaints[order(noise_complaints$Date), ]

#noise_complaints$n_adj_diff<-c(NA, diff(noise_complaints$n_adj))
noise_complaints$log_n_adj_diff<-c(NA, diff(noise_complaints$log_n_adj))
noise_complaints$log_n_adj_diff2 <- c(NA, diff(noise_complaints$log_n_adj_diff))


# test-train split
last_value<-noise_complaints[nrow(noise_complaints),]
noise_complaints<-noise_complaints[-1,]

```

After the seasonality adjustment, we also remove the last value $x_{n+1}$ of the dataset for validation purposes. This way, we can check the performance of each model we run (i.e. ARIMA vs. ARIMA-ARCH). The log of the final value (April 30) is 5.76 and the seasonally adjusted final value is $x_{n+1}=-0.42$.

Now, we can view the seasonally adjusted series and the difference of the series again now that we have removed the weekly structure:

```{r, echo=FALSE, fig.width=8, fig.height=5}
# four plots again
par(mfrow=c(2,1))
# series, diff series
plot(noise_complaints$Date, noise_complaints$log_n_adj, type = 'l',  main = 'Log Daily Noise Complaints Adj.', xlab = 'Date', ylab='Log Noise Complaints Adj.', col='red')
plot(noise_complaints$Date, noise_complaints$log_n_adj_diff, type = 'l',  main = 'Log Diff. Daily Noise Complaints Adj.', xlab = 'Date', ylab='Log Diff. Noise Complaints Adj.', col='red')
```

Viewing the seasonally adjusted data, we can see that the data is mostly mean reverting around approximately 0. However, there was a surge in noise complaints and volatility around the time period when COVID started. Thus, we can check the ACF and PACF plots for values of d between 0 and 2 to select a value for the parameter:

```{r, fig.width=8, fig.height=6}

par(mfrow=c(3,2))
# n
Acf(noise_complaints$log_n_adj, main='ACF: Noise Complaints Adj. (d=0)', lag.max = 365) 
Pacf(noise_complaints$log_n_adj, main='PACF: Noise Complaints Adj. (d=0)', lag.max = 365)
# diff n
Acf(noise_complaints$log_n_adj_diff, main='ACF: Diff. Noise Complaints Adj. (d=1)', lag.max = 365) 
Pacf(noise_complaints$log_n_adj_diff, main='PACF: Diff. Noise Complaints Adj. (d=1)', lag.max = 365)
#par(mfrow=c(3,2))
# diff2 n - overdifferenced
Acf(noise_complaints$log_n_adj_diff2, main='ACF: Diff2 Noise Complaints Adj. (d=2)', lag.max = 365) 
Pacf(noise_complaints$log_n_adj_diff2, main='PACF: Diff2 Noise Complaints Adj. (d=2)', lag.max = 365)



```

The main purpose of differencing is to make the data stationary. We want to make sure to difference only as many times as is necessary. We wish to use an integer value for d, so there is a tough decision to make between d=0 and d=1.  We will select d=0 for the following reasons: There is a long die down in the series' ACF plot, which could indicate long memory. Furthermore, the die down does not start at autocorrelation=1. Another reason is that the ACF plot of the difference (d=1) starts out around -0.3, which is approaching -0.5. <br/>

```{r}
#fractional differencing:
fracdiff::fracdiff(noise_complaints$log_n_adj)
# the value of d could be estimated in ARFIMA as 0.4

#tseries::adf.test(noise_complaints$log_n_adj) # do not reject the null, don't difference
# tseries::adf.test(noise_complaints$log_n_adj, 
#                   k = trunc((length(noise_complaints$log_n_adj)-1)^(1/3)),
#                   alternative = 'stationary')
# 
# tseries::kpss.test(noise_complaints$log_n_adj_diff, null = c("Level", "Trend"))

#print(paste0('The number of differences according to the ndiffs function in R, which computs an adf test at the 0.05 level: ', forecast::ndiffs(noise_complaints$log_n_adj, test="adf", alpha = 0.05, max.d = 2, type = 'level')))
```

Should we wish to use a d between 0 and 1, we can run the `fracdiff::fracdiff` command (above) which generated a d of 0.4, which is closer to 0 than 1. Finally, I tried forecast::ndiffs to help make my decision with the result of 0. For these reasons and that we want to be conservative and guard against overdifferencing, we select d=0. *Note: Even though we adjusted the data, it seems like there is still some seasonality structure in the data (each 7 days), so there may be better ways to improve our model (i.e. use SARIMA). This may be another reason that differencing is not straightforward in this case. *

By selecting d=0, we will assume that the data is stationary. While in practice, it is difficult to find a truly stationary series, it is the best assumption we can make at this time. We use these plots to identify the d for our modeling, but we cannot use them to identify an ARIMA(p,d,q). Instead, we can use AICC as a metric to select an ARIMA model. 

# 2. Arima Model Selection Using AICc

We can use the AICc criteria to select which ARIMA(p, 0, q) model we should choose:

```{r}

# model selection loop
possible_p<-0:2
possible_q<-0:2

aicc_results <- data.frame(p=character(),
                           d=character(),
                           q=character(), 
                           constant=character(), 
                           aicc=numeric()) 

# model selection among candidates
i=0
for(p in possible_p){
        for(q in possible_q){
          
                # constant
                i<-i+1
                #print(paste0('Running: ARIMA(', p, ', 0, ', q, ' with constant.)'))
                aicc_results[i, 'p']<-p
                aicc_results[i, 'd']<-0
                aicc_results[i, 'q']<-q
                aicc_results[i, 'constant']<-TRUE
                aicc_results[i, 'aicc']<- Arima(noise_complaints$log_n_adj, c(p, 0, q), include.constant=T)$aicc
                # no constant
                i<-i+1
                #print(paste0('Running: ARIMA(', p, ', 1, ', q, ' without constant.)'))
                aicc_results[i, 'p']<-p
                aicc_results[i, 'd']<-0
                aicc_results[i, 'q']<-q
                aicc_results[i, 'constant']<-FALSE
                aicc_results[i, 'aicc']<-Arima(noise_complaints$log_n_adj, c(p, 0, q), include.constant=F)$aicc
                
        }
}

aicc_results$aicc <- round(aicc_results$aicc, 2)
datatable(aicc_results[order(aicc_results$aicc),], 
          options = list(pageLength = nrow(aicc_results)))

# select the arima(1,0,2) with AICc=-190.2
```

We select the ARIMA(1, 0, 2) with no constant and the minimum AICc=-190.2 among candidates. The model summary is printed below:

```{r}
# model selected fit
# fit_select <- Arima(noise_complaints$n_adj_diff, c(2,0,1), include.constant=F, method="ML")
# summary(fit_select)

# model for forecasting
fit.mean<-Arima(noise_complaints$log_n_adj, c(1, 0, 2), include.constant=F, method="ML")
summary(fit.mean)
```

The formula of the model we fitted is is $x_t = 0.9969{t-1}  + \varepsilon_{t} - 0.5880\varepsilon_{t-1} - 0.3158\varepsilon_{t-2}$, where $x_t$ is the number of daily adjusted residential noise complaints in NYC. *Note: In Minitab, the MA coefficients have the sign flipped compared to the R output, so we don’t need to change the sign here.*

```{r}
# residuals
resid <- residuals(fit.mean)
#tail(resid, n=10)
# fitted values
f <- fitted.values(fit.mean)
#tail(f, n=10)
```

The one-step ahead forecast of this model is, including 95% confidence intervals: <br/>

```{r}
# one-step ahead forecast
fcasts<-forecast(fit.mean, h = 1) %>% as.data.frame()

datatable(fcasts[, c(1, 4, 5)] %>% round(., 3))

# real value 
#print(paste0('The real value at time n+1 is: ',  last_value$log_n_adj %>% round(.,2)))
```

While this forecast interval seems large, it needs to be this large to fit the data 95% of the time. The interval does contain the real value, -0.418, for the one-step ahead prediction (see problem 10).

# 3. Arima Residuals Analysis

Here is a plot of the residuals as well as ACF and PACF of both the residuals and the squared residuals:

```{r, fig.width=8, fig.height=6}
par(mfrow=c(3,2))
# plot of the residuals
plot(noise_complaints$Date, resid, type="l",
     xlab="Date", ylab="Residuals Fitted", col='red')
plot(noise_complaints$Date, resid^2, type="l",
     xlab="Date", ylab="Squared Residuals Fitted", col='red')

# acf, pacf of residuals
Acf(resid, main='ACF: Residuals')
Pacf(resid, main='PACF: Residuals')

Acf(resid^2, main='ACF: Residuals Sq.')
Pacf(resid^2, main='PACF: Residuals Sq.')
```

The ACF and PACF plots of the residuals do not show significant autocorrelations and partial autocorrelations especially at low lags; thus, they appear to be uncorrelated. There does appear to be some pattern at ~7 days between lags, which may be associated with the day of week which has some structure even though we made a seasonality adjustment. Even if the residuals are uncorrelated and centered at zero (the expectation is zero), they do not look to be independent from the plots of the residuals and squared residuals. The residuals squared ACF plot seems to die down, which might be an indication that there is conditional volatility in our data that could be captured using a different model (i.e. ARCH). More specifically, when the volatility is high, the ARIMA model fits less well. This problem is evidence of conditional heteroscedasticity since when volatility is high, it tends to stay high and when it is low it tends to stay low. 

# 4. ARCH Model Selection & GARCH(1,1)

Using the residuals from the ARIMA model, we find the log likelihood values and AICC values for ARCH(q) models where q ranges from 0 to 10. The log likelihood for the ARCH(0) model is calculated by hand. Here are the AICc values for the ARCH(q): <br/>

```{r}
q <- 0:10
loglik <- rep(NA, length(q))
N <- length(resid)

for (i in 1:length(q)) {
        if (q[i] == 0) {
                
                # Calculate the log likelihood for the ARCH(0) model by hand. 
                #See the handout on Estimation and Automatic Selection of ARCH models.
                
                loglik[i] <- -0.5 * N * (1 + log(2 * pi * mean(resid^2)))
                
        } else {
                fit <- garch(resid, c(0,q[i]), trace=FALSE)
                loglik[i] <- logLik(fit)
        }
}


k <- q + 1
aicc <- -2 * loglik  + 2 * k * N / (N - k - 1)

aicc2<-data.frame(q, loglik, aicc)
datatable(aicc2[order(aicc2$aicc, decreasing = F),] %>% round(., 2), options = list(pageLength = nrow(aicc2)))

# select ARCH 8 with aicc = -528.34
```
The best model based on q ranges from 0 to 10 is ARCH(8), which has the lowest AICC of -528.34. <br/>

Next, we consider a GARCH (1,1) model and evaluate AICC for the GARCH (1,1) model, using q=2.  <br/>

```{r}
# try garch 1, 1
fit <- garch(resid, c(1,1), trace=FALSE)
loglik <- logLik(fit)
k <- 2
aicc <- -2 * loglik  + 2 * k * N / (N - k - 1)
datatable(data.frame(loglik, aicc) %>% round(., 2))
# aicc = -494.24
```

We could select the model with the lowest AICc=-528.34, which is the ARCH(8) model. The model results below indicate that omega and some alphas (lower from 1-2 and weekly from 6-8) are significant: 

```{r}
# select the arch model since it has the lowest AICC
#fit.var <- garch(resid, c(1,1), trace=FALSE)
fit.var <- garch(resid, c(0,9), trace=FALSE)
summary(fit.var)
logLik(fit.var)
```

However, since the ARCH model is not parsimonious and many coefficients are not significant, the GARCH(1,1) model may be a more simple model that we could use for similar results in forecasting. Furthermore, the GARCH(1,1) model has three coefficients that are very statistically significant p<2e-16:


```{r}
# select the arch model since it has the lowest AICC
#fit.var <- garch(resid, c(1,1), trace=FALSE)
fit.var <- garch(resid, c(1,1), trace=FALSE)
summary(fit.var)
logLik(fit.var)

```

In this case, we prefer the more parsimonious model. Thus, the selected model has the form: $h_t = 0.014210 + 0.434731\varepsilon^2_{t-1} + 0.368945 h_{t-1}$. Based on the model output, all estimates are statistically significance with p<2e-16. *Note: The parameters are significant based on the above summary output. Since the output gives us the two-sided p-values, we should divide the given p-values by two to get the one-sided value. This further signifies the significance of these parameters. However, we should still be wary of putting too much stake in the statistical significance of these parameters since we are most focused on getting the best forecast rather than the interpretability of these coefficients.* <br/>

While the Box-Ljung test p-value is high so the model seems to fit based on this criteria, the model fails the test for the residuals. There may be a better fitting model that we could use to capture this variation. <br/>

The unconditional (marginal) variance of the shocks can be computed with the formula $\text{var}(\varepsilon_t) = \frac{\omega}{1-\sum_{j=1}^{q}\hat{\alpha_j}}$. So, we can compute our unconditional variance as $\frac{0.014210}{1-(0.434731+0.368945)} = \frac{0.014210}{0.196324} = 0.07238035$.



# 5. Forecast of ARIMA-ARCH model

Construct a 95% one step ahead forecast interval for the log adj. noise complaints, based on your ARIMA-ARCH model. We use the formula $h_t = f_1 + \sqrt(h_1)$, where $h_1$ is given by the model formula above and $f_1$ is the ARIMA forecast.

```{r, echo=T}
f1 <- fcasts$`Point Forecast`

ht <- fit.var$fit[,1]^2 
h1 <- fit.var$coef[1] + tail(ht, n=1) %>% as.numeric()

f1 + c(-1, 1) * 1.96 * sqrt(h1)

```

```{r}
# real value 
#print(paste0('The real value at time n is: ',  last_value$log_n_adj %>% round(.,2)))

# check: ht-1
# 1 NA from the difference of the series
# 1 NA from ht GARCH
resid.arch <- c(NA, resid[-1]) / sqrt(c(NA, ht[-length(ht)]))
#sum(is.na(resid.arch))
#length(resid.arch)
```

This interval is wider than the interval from problem 2 which was thinner. Our second model may be able to better capture the volatility so that we can be more sure during the more volatile time period and still be 95% confident in our forecast.  Since the volatility lately has been higher (perhaps due to COVID), the ARIMA-ARCH forecast has a wider interval to accommodate it. <br/>

The 95% confidence interval is actually computing the 2.5% and 97.5% percentiles to get 5% of the data outside of the center. To get the 5th percentile we need to do the same operation as we did for 95% but with a different z-score. We use 1.96 for 95% and 1.65 for 90% confidence intervals: 

```{r}
f1 + c(-1, 1) * 1.65 * sqrt(h1)
```

# 6. Conditional Variances Analysis:

Here are the conditional variances, ht, for the fitted ARCH model from problem 4.

```{r, fig.width=8, fig.height=6}
par(mfrow=c(2,1))
ht <- fit.var$fit[,1]^2
plot(noise_complaints$Date, ht, type="l", col=4, xlab='Date', main = 'Conditional Variances')

# like residuals squared but smoothed
```

Volatility is especially high at the middle and end of years, which may correspond to July 4 celebrations and New Years celebrations. Prior to 2020, the volatility tended to be lower while in more recent times the volatility has been higher, perhaps due to COVID-19 and residents staying home more. The plot above seems to follow the same pattern as the time series plot in terms of volatility (problem #1). It also appears to capture a smoothed version of the residual plot from #3. <br/>


# 7. Visualize the ARIMA-ARCH Forecast

Here is a time series plot which simultaneously shows the log adj. noise complaints, together with the ARIMA-ARCH one-step-ahead 95% forecast intervals based on information available in the previous day: <br/>

```{r, fig.width=8, fig.height=6}
par(mfrow=c(2,1))
plot(noise_complaints$Date, noise_complaints$log_n_adj, type="l", xlab='Date', main = 'ARIMA-ARCH Forecast', ylab='Noise Complaints Adj.')
lines(noise_complaints$Date, f + 1.96 * sqrt(ht), lty=2, col=2)
lines(noise_complaints$Date, f - 1.96 * sqrt(ht), lty=2, col=2)

```
While the forecast interval may seem wide, the past fitted values seem close together. The 95% interval follows the time series closely and appears to have only a few misses (when the interval does not capture the real data point). It appears that the forecast interval needs to be this wide to make sure we are able to forecast correctly 95% of the time, especially to capture the innate volatility of the dataset. 

# 8. ARIMA-ARCH Model Adequecy

The residuals from your ARIMA-ARCH model are $e_t = \varepsilon_t /\sqrt{h_t}$. If the ARIMA-ARCH model is adequate, these residuals should be normally distributed with mean zero and variance 1. Here is a normal probability plot of the ARCH residuals:

```{r, fig.width=8, fig.height=6}
par(mfrow=c(2,1))
#resid.arch <- fit.var$residuals
qqnorm(resid.arch, col=2)
qqline(resid.arch, col=1, lty=2)

```

There are a large number of data points that lie off of the x=y line towards the right and left side of the plot (tails of the distribution). This suggests that the model did not adequately describe the leptokurtosis in our data. If the model described it well, the data would sit more on the x=y line (normal data). 

# 9. Forecast Interval Assessment

We can compare the forecasts on the same plot, where the main difference is that during periods of high volatility, the ARIMA-ARCH forecast is wider than the ARIMA forecast and narrower during low volatility:

```{r, fig.width=8, fig.height=6}
par(mfrow=c(2,1))

ft <- fitted.values(fit.mean)
ht <- fit.var$fit[,1]^2 
sigma2 <- fit.mean$sigma2

plot(noise_complaints$Date, noise_complaints$log_n_adj, type="l", col=2, xlab='Date', main = 'Forecasts', ylab='Noise Complaints Adj.')

# ARIMA intervals
lines(noise_complaints$Date, ft + sqrt(sigma2) * 1.96, col=3)
lines(noise_complaints$Date, ft - sqrt(sigma2) * 1.96, col=3)

# ARIMA-ARCH intervals
lines(noise_complaints$Date, ft + sqrt(ht) * 1.96, col=4)
lines(noise_complaints$Date, ft - sqrt(ht) * 1.96, col=4)

legend("topleft", inset=0.05,
       legend=c("Series", "ARIMA", "ARIMA-ARCH"), lty=1, col=2:4, cex = 0.4)
# compare 
```

We can compute the percentage of times we miss the intervals using the past data (abs(resid)>1.96). It is close to 5% ($\approx 4.78%$ for ARIMA-ARCH, $\approx 5.70%$ for ARIMA), so our interval is approximately wide enough in both cases. Capturing the volatility has allowed us to reach the 5% threshold and improved the forecast compared to the ARIMA only model. 

```{r, echo=F}

# compute the number of misses
print(paste0("Percentage missed ARIMA-ARCH: " , 
             round(
               sum(abs(fit.var$residuals) > 1.96, na.rm=TRUE)  /
                     sum(!is.na(fit.var$residuals)) * 100, 2)
             )
      )

print(paste0("Percentage missed ARIMA: ",
             round(
               sum(abs(noise_complaints$log_n_adj - ft) > sqrt(sigma2) * 1.96, na.rm=TRUE) / 
                 sum(!is.na(noise_complaints$log_n_adj)) * 100, 2)
             )
      )


```


# 10. Predict and Compare 1-step ahead by Model:

```{r}
# Add part 10: predicting x_{n+1}
print(paste0("The real last data point is : " , 
             last_value$log_n_adj %>% round(.,2), '.'
      ))
```

The last point is -0.418 which is contained in both the ARIMA and ARIMA-ARCH intervals (ARIMA = c(-0.462, 0.440) and ARIMA-ARCH = c( -0.627,  0.605)). Even though the forecast intervals for the ARIMA-ARCH model are wider around the actualized values of noise complaints, we miss fewer times compared to the ARIMA. This might indicate that adding volatility explanation to our model with an ARCH model, we have gotten closer to the structure of the true data. Since the volatility remains high during COVID, the ARIMA-ARCH prediction interval remains wider than the ARIMA interval.